{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook replicates the experimental setup used in the comparison presented in the corresponding Esann paper found here:\n",
    "https://pub.uni-bielefeld.de/publication/2908201\n",
    "\n",
    "requires borutyPy:\n",
    "https://github.com/scikit-learn-contrib/boruta_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import  RidgeClassifier, ElasticNet, SGDClassifier\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_classif, GenericUnivariateSelect\n",
    "from rbclassifier import RelevanceBoundsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "class FSmodel(object):\n",
    "    \"\"\"\n",
    "    Abstract class for all models which are used for feature selection\n",
    "    \"\"\"\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    def __init__(self,X,Y):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "        scaler = StandardScaler().fit(self.X_train)\n",
    "        self.X_train = scaler.transform(self.X_train)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "    @abc.abstractmethod\n",
    "    def predict(self):\n",
    "        return\n",
    "\n",
    "\n",
    "class SVCL1(FSmodel):\n",
    "    def predict(self):\n",
    "        clf = LinearSVC(penalty=\"l1\",dual=False).fit(self.X_train, self.y_train)\n",
    "        sfm = SelectFromModel( clf, prefit=True)\n",
    "        self.score = clf.score(self.X_test,self.y_test)\n",
    "        self.dec = clf.coef_[0]\n",
    "        return   sfm.get_support()\n",
    "class Ridge(FSmodel):\n",
    "    def predict(self):\n",
    "        clf = RidgeClassifier().fit(self.X_train, self.y_train)\n",
    "        sfm = SelectFromModel(clf, prefit=True)\n",
    "        self.score = clf.score(self.X_test,self.y_test)\n",
    "        self.dec = clf.coef_[0]\n",
    "        return sfm.get_support()\n",
    "class ElasticN(FSmodel):\n",
    "    def predict(self):\n",
    "        tuned_parameters = [{'alpha': 0.00001 * np.logspace(0, 3)}]\n",
    "        clf = GridSearchCV(SGDClassifier(l1_ratio=0.15,penalty=\"elasticnet\"),\n",
    "                           tuned_parameters,\n",
    "                            cv=5)\n",
    "        clf.fit(self.X_train, self.y_train)\n",
    "        clf = clf.best_estimator_\n",
    "        sfm = SelectFromModel(clf, prefit=True)\n",
    "        self.score = clf.score(self.X_test,self.y_test)\n",
    "        self.dec = clf.coef_[0]\n",
    "        return sfm.get_support()\n",
    "class BorutaModel(FSmodel):\n",
    "    def predict(self):\n",
    "        rf = RandomForestClassifier(max_depth=5)\n",
    "        borutaf = BorutaPy(rf, n_estimators=\"auto\", verbose=False)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', 'invalid value encountered in greater')\n",
    "            borutaf.fit(self.X_train, self.y_train)\n",
    "        rf = RandomForestClassifier(max_depth=3).fit(self.X_train, self.y_train)\n",
    "        self.score = rf.score(self.X_test,self.y_test)\n",
    "        self.dec = np.zeros(len(self.X_train.T))\n",
    "        return borutaf.support_\n",
    "\n",
    "class OurMethod(FSmodel):\n",
    "    def predict(self):\n",
    "        self.smo = RelevanceBoundsClassifier()\n",
    "        self.smo.fit(self.X_train, self.y_train)\n",
    "        self.score = self.smo._svm_clf.score(self.X_test,self.y_test)\n",
    "        self.dec =np.zeros(len(self.X_train.T))\n",
    "        return self.smo.allrel_prediction_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rbclassifier.genData import genData\n",
    "import numpy as np\n",
    "\n",
    "def data(n=150, d=12, redundant=4, informative=3, repeated = 0, state=1338):\n",
    "    randomstate = np.random.RandomState(state)\n",
    "    # Use method provided my library\n",
    "    X1, Y1 = genData(n_samples=n, n_features=d,\n",
    "                           n_redundant=redundant,\n",
    "                           strRel=informative, n_repeated=repeated,\n",
    "                           class_sep=0.3,\n",
    "                           flip_y=0)\n",
    "    # Get a truth vector for accuracy measurement\n",
    "    truth = [True]*(informative+redundant) + [False]*(d-(informative+redundant))\n",
    "    strTruth = [True]*(informative) + [False]*(d-(informative))\n",
    "    weakTruth  = [False]*(informative) + [True]*(redundant) + [False]*(d-(informative+redundant))\n",
    "    return X1,Y1,truth, strTruth,weakTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define which methods to compare\n",
    "tests = [Ridge,SVCL1,ElasticN,OurMethod,BorutaModel] # Use model names from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,roc_auc_score, classification_report, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import itertools \n",
    "\n",
    "\n",
    "test_names =  [x.__name__ for x in tests]\n",
    "test_metrics  = [\"accuracy\",\"precision\",\"recall\",\"F1 measure\",\"auc\"]\n",
    "test_n = 10\n",
    "test_range = list(range(1337,1337+test_n))\n",
    "\n",
    "\n",
    "\n",
    "def singleTest(X1,Y1,truth):\n",
    "        out =  []\n",
    "        for x in tests:\n",
    "            model = x(X1,Y1)\n",
    "            pred = model.predict()      \n",
    "            prec = precision_score(truth,pred)\n",
    "            recall = recall_score(truth,pred)\n",
    "            F1 = f1_score(truth,pred)\n",
    "            accuracy = model.score\n",
    "            auc = roc_auc_score(truth, model.dec)\n",
    "            out.append([accuracy,prec,recall,F1,auc])\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run feature selection with various models\n",
    "test is repeated multple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for all relevant feature selection performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a pandas Panel to hold results for later\n",
    "allRelData = pd.Panel(items=test_names, major_axis=test_range, minor_axis=test_metrics)\n",
    "\n",
    "# Generate data for experiment\n",
    "all_rel_test_data = []\n",
    "for state in test_range:\n",
    "    all_rel_test_data.append(data(n=150, d=12, redundant=4, informative=3, repeated = 0, state=state))\n",
    "    \n",
    "    \n",
    "# Run single tests in parallel using pool    \n",
    "with Pool() as p:\n",
    "    data_iter = [all_rel_test_data[xi][0:3] for xi in range(test_n)]\n",
    "    parall_result = p.starmap(singleTest,data_iter)\n",
    "        \n",
    "# Save results in pandas.Panel        \n",
    "for i,state in zip(test_range,parall_result):\n",
    "    for j,x in zip(range(len(tests)),tests):\n",
    "        allRelData.loc[x.__name__].loc[i] = state[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for strongly relevant feature selection performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strRelData = pd.Panel(items=test_names, major_axis=test_range, minor_axis=test_metrics)\n",
    "str_rel_test_data = []\n",
    "for state in test_range:\n",
    "    str_rel_test_data.append( data(n=150, d=12, redundant=0, informative=6, repeated = 0, state=state))\n",
    "    \n",
    "with Pool() as p:\n",
    "    data_iter = [str_rel_test_data[xi][0:3] for xi in range(test_n)]\n",
    "    parall_result = p.starmap(singleTest,data_iter)\n",
    "        \n",
    "# Save results in pandas.Panel        \n",
    "for i,state in zip(test_range,parall_result):\n",
    "    for j,x in zip(range(len(tests)),tests):\n",
    "        strRelData.loc[x.__name__].loc[i] = state[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for weakly relevant feature selection performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weakRelData = pd.Panel(items=test_names, major_axis=test_range, minor_axis=test_metrics)\n",
    "\n",
    "weak_rel_test_data = []\n",
    "for state in test_range:\n",
    "    weak_rel_test_data.append(  data(n=150, d=12, redundant=6, informative=0, repeated = 0, state=state))\n",
    "    \n",
    "with Pool() as p:\n",
    "    data_iter = [weak_rel_test_data[xi][0:3] for xi in range(test_n)]\n",
    "    parall_result = p.starmap(singleTest,data_iter)\n",
    "        \n",
    "# Save results in pandas.Panel        \n",
    "for i,state in zip(test_range,parall_result):\n",
    "    for j,x in zip(range(len(tests)),tests):\n",
    "        weakRelData.loc[x.__name__].loc[i] = state[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information from matlab ( ignore the block if matlab code is missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using external matlab implementation for mutual information from Benoît Frénay (https://bfrenay.wordpress.com/mutual-information/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save datasets for matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "for state,datum in zip(test_range,str_rel_test_data):\n",
    "    X1, Y1, *_ = datum\n",
    "    obj_arr = {\"x\":X1,\"y\":Y1}\n",
    "    sio.savemat(\"../benoit/datasets/s{}.mat\".format(state),obj_arr)\n",
    "for state,datum in zip(test_range,weak_rel_test_data):\n",
    "    X1, Y1, *_ = datum\n",
    "    obj_arr = {\"x\":X1,\"y\":Y1}\n",
    "    sio.savemat(\"../benoit/datasets/w{}.mat\".format(state),obj_arr)\n",
    "for state,datum in zip(test_range,all_rel_test_data):\n",
    "    X1, Y1, *_ = datum\n",
    "    obj_arr = {\"x\":X1,\"y\":Y1}\n",
    "    sio.savemat(\"../benoit/datasets/a{}.mat\".format(state),obj_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN MATLAB....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import results back to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weak_mat_results = pd.Panel(items=test_range, minor_axis=range(12), major_axis=['rel_lower_MI_bounds', 'rel_upper_MI_bounds','rel_lower_MSE_bounds','rel_upper_MSE_bounds'])\n",
    "strong_mat_results = pd.Panel(items=test_range, minor_axis=range(12), major_axis=['rel_lower_MI_bounds', 'rel_upper_MI_bounds','rel_lower_MSE_bounds','rel_upper_MSE_bounds'])\n",
    "all_mat_results = pd.Panel(items=test_range, minor_axis=range(12), major_axis=['rel_lower_MI_bounds', 'rel_upper_MI_bounds','rel_lower_MSE_bounds','rel_upper_MSE_bounds'])\n",
    "\n",
    "\n",
    "for state in test_range:\n",
    "    mat = sio.loadmat(\"../benoit/ares{}.mat\".format(state))\n",
    "    all_mat_results.loc[state] = mat['rel_lower_MI_bounds'],  mat['rel_upper_MI_bounds'],  mat['rel_lower_MSE_bounds'],  mat['rel_upper_MSE_bounds']\n",
    "    \n",
    "    mat = sio.loadmat(\"../benoit/sres{}.mat\".format(state))\n",
    "    strong_mat_results.loc[state] = mat['rel_lower_MI_bounds'],  mat['rel_upper_MI_bounds'],  mat['rel_lower_MSE_bounds'],  mat['rel_upper_MSE_bounds']\n",
    "    \n",
    "    mat = sio.loadmat(\"../benoit/wres{}.mat\".format(state))\n",
    "    weak_mat_results.loc[state] = mat['rel_lower_MI_bounds'],  mat['rel_upper_MI_bounds'],  mat['rel_lower_MSE_bounds'],  mat['rel_upper_MSE_bounds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict feature relevancies for MI-results using thresholds analogue to our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'str_rel_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6153ee92f0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr_rel_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mtruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmirange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_mat_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str_rel_test_data' is not defined"
     ]
    }
   ],
   "source": [
    "def predict(rangevector):\n",
    "        prediction = np.zeros(len(rangevector))\n",
    "        # Threshold for relevancy\n",
    "        upper_epsilon = 0.1\n",
    "        lower_epsilon = 0.05\n",
    "\n",
    "        # Weakly relevant ones have high upper bounds\n",
    "        prediction[rangevector[:, 1] > upper_epsilon] = 1\n",
    "        # Strongly relevant bigger than 0 + some epsilon\n",
    "        prediction[rangevector[:, 0] > lower_epsilon] = 2\n",
    "\n",
    "        allrel_prediction = prediction.copy()\n",
    "        allrel_prediction[allrel_prediction == 2] = 1\n",
    "\n",
    "        return allrel_prediction, prediction\n",
    "    \n",
    "def decision_function2(rangevector):\n",
    "        upper_epsilon = 0.1\n",
    "        lower_epsilon = 0.05\n",
    "        dec1 = rangevector[:,0]-lower_epsilon\n",
    "        dec2 = rangevector[:,1]-upper_epsilon\n",
    "\n",
    "        return dec1\n",
    "out = []\n",
    "for state,datum in zip(test_range,str_rel_test_data):\n",
    "    truth = datum[2]\n",
    "    mirange = all_mat_results.loc[state].iloc[2:4].T.values\n",
    "    pred,_ = predict(mirange)\n",
    "    prec = precision_score(truth,pred)\n",
    "    recall = recall_score(truth,pred)\n",
    "    F1 = f1_score(truth,pred)\n",
    "    accuracy = 0\n",
    "    auc = roc_auc_score(truth, decision_function2(mirange))\n",
    "    out.append([accuracy,prec,recall,F1,auc])\n",
    "allRelData.loc[\"MutualInformation\"] = np.array(out)\n",
    "out = []\n",
    "for state,datum in zip(test_range,str_rel_test_data):\n",
    "    truth = datum[2]\n",
    "    mirange = strong_mat_results.loc[state].iloc[2:4].T.values\n",
    "    pred,_ = predict(mirange)\n",
    "    prec = precision_score(truth,pred)\n",
    "    recall = recall_score(truth,pred)\n",
    "    F1 = f1_score(truth,pred)\n",
    "    accuracy = 0\n",
    "    auc = roc_auc_score(truth, decision_function2(mirange))\n",
    "    out.append([accuracy,prec,recall,F1,auc])\n",
    "strRelData.loc[\"MutualInformation\"] = np.array(out)\n",
    "out = []\n",
    "for state,datum in zip(test_range,str_rel_test_data):\n",
    "    truth = datum[2]\n",
    "    mirange = weak_mat_results.loc[state].iloc[2:4].T.values\n",
    "    pred,_ = predict(mirange)\n",
    "    prec = precision_score(truth,pred)\n",
    "    recall = recall_score(truth,pred)\n",
    "    F1 = f1_score(truth,pred)\n",
    "    accuracy = 0\n",
    "    auc = roc_auc_score(truth, decision_function2(mirange))\n",
    "    out.append([accuracy,prec,recall,F1,auc])\n",
    "weakRelData.loc[\"MutualInformation\"] = np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for dataset with only strongly relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1 measure</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.942222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.903636</td>\n",
       "      <td>0.511111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVCL1</th>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.569394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724608</td>\n",
       "      <td>0.516667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticN</th>\n",
       "      <td>0.911111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.934545</td>\n",
       "      <td>0.508333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OurMethod</th>\n",
       "      <td>0.962222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.943636</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BorutaModel</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.808368</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall  F1 measure       auc\n",
       "Ridge        0.942222   1.000000  0.833333    0.903636  0.511111\n",
       "SVCL1        0.977778   0.569394  1.000000    0.724608  0.516667\n",
       "ElasticN     0.911111   1.000000  0.883333    0.934545  0.508333\n",
       "OurMethod    0.962222   1.000000  0.900000    0.943636  0.500000\n",
       "BorutaModel  0.800000   0.985714  0.716667    0.808368  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(strRelData.mean().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for dataset with only weakly relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1 measure</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.862727</td>\n",
       "      <td>0.369444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVCL1</th>\n",
       "      <td>0.984444</td>\n",
       "      <td>0.569091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724510</td>\n",
       "      <td>0.358333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticN</th>\n",
       "      <td>0.975556</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.892727</td>\n",
       "      <td>0.358333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OurMethod</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BorutaModel</th>\n",
       "      <td>0.877778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall  F1 measure       auc\n",
       "Ridge        0.955556   0.980000  0.800000    0.862727  0.369444\n",
       "SVCL1        0.984444   0.569091  1.000000    0.724510  0.358333\n",
       "ElasticN     0.975556   0.980000  0.833333    0.892727  0.358333\n",
       "OurMethod    0.973333   1.000000  0.966667    0.980000  0.500000\n",
       "BorutaModel  0.877778   1.000000  0.866667    0.920000  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(weakRelData.mean().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark for all relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1 measure</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.948889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.816061</td>\n",
       "      <td>0.594286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVCL1</th>\n",
       "      <td>0.971111</td>\n",
       "      <td>0.657576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.792466</td>\n",
       "      <td>0.617143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticN</th>\n",
       "      <td>0.946667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.854429</td>\n",
       "      <td>0.597143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OurMethod</th>\n",
       "      <td>0.962222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.951282</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BorutaModel</th>\n",
       "      <td>0.804444</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.858625</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             accuracy  precision    recall  F1 measure       auc\n",
       "Ridge        0.948889   1.000000  0.700000    0.816061  0.594286\n",
       "SVCL1        0.971111   0.657576  1.000000    0.792466  0.617143\n",
       "ElasticN     0.946667   1.000000  0.757143    0.854429  0.597143\n",
       "OurMethod    0.962222   1.000000  0.914286    0.951282  0.500000\n",
       "BorutaModel  0.804444   0.980000  0.771429    0.858625  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(allRelData.mean().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for paper in latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  precision &  recall &  F1 measure \\\\\n",
      "\\midrule\n",
      "Ridge       &       1.00 &    0.83 &        0.90 \\\\\n",
      "SVCL1       &       0.57 &    1.00 &        0.72 \\\\\n",
      "ElasticN    &       1.00 &    0.88 &        0.93 \\\\\n",
      "OurMethod   &       1.00 &    0.90 &        0.94 \\\\\n",
      "BorutaModel &       0.99 &    0.72 &        0.81 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  precision &  recall &  F1 measure \\\\\n",
      "\\midrule\n",
      "Ridge       &       0.98 &    0.80 &        0.86 \\\\\n",
      "SVCL1       &       0.57 &    1.00 &        0.72 \\\\\n",
      "ElasticN    &       0.98 &    0.83 &        0.89 \\\\\n",
      "OurMethod   &       1.00 &    0.97 &        0.98 \\\\\n",
      "BorutaModel &       1.00 &    0.87 &        0.92 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  precision &  recall &  F1 measure \\\\\n",
      "\\midrule\n",
      "Ridge       &       1.00 &    0.70 &        0.82 \\\\\n",
      "SVCL1       &       0.66 &    1.00 &        0.79 \\\\\n",
      "ElasticN    &       1.00 &    0.76 &        0.85 \\\\\n",
      "OurMethod   &       1.00 &    0.91 &        0.95 \\\\\n",
      "BorutaModel &       0.98 &    0.77 &        0.86 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = [strRelData,weakRelData,allRelData]\n",
    "for dataset in [strRelData,weakRelData,allRelData]:\n",
    "    latex  = dataset.mean().T.drop([\"accuracy\",\"auc\"],axis=1).round(2).to_latex()\n",
    "    print(latex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrr}\n",
      "\\toprule\n",
      "{} & Dataset 1 &        &       & Dataset 2 &        &       & Dataset 3 &        &       \\\\\n",
      "{} & precision & recall &    F1 & precision & recall &    F1 & precision & recall &    F1 \\\\\n",
      "\\midrule\n",
      "Ridge       &      1.00 &   0.83 &  0.90 &      0.98 &   0.80 &  0.86 &      1.00 &   0.70 &  0.82 \\\\\n",
      "SVCL1       &      0.57 &   1.00 &  0.72 &      0.57 &   1.00 &  0.72 &      0.66 &   1.00 &  0.79 \\\\\n",
      "ElasticN    &      1.00 &   0.88 &  0.93 &      0.98 &   0.83 &  0.89 &      1.00 &   0.76 &  0.85 \\\\\n",
      "OurMethod   &      1.00 &   0.90 &  0.94 &      1.00 &   0.97 &  0.98 &      1.00 &   0.91 &  0.95 \\\\\n",
      "BorutaModel &      0.99 &   0.72 &  0.81 &      1.00 &   0.87 &  0.92 &      0.98 &   0.77 &  0.86 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = strRelData.mean().T.drop([\"accuracy\",\"auc\"],axis=1).rename(columns={\"F1 measure\":\"F1\"})\n",
    "d2 = weakRelData.mean().T.drop([\"accuracy\",\"auc\"],axis=1).rename(columns={\"F1 measure\":\"F1\"})\n",
    "d3 = allRelData.mean().T.drop([\"accuracy\",\"auc\"],axis=1).rename(columns={\"F1 measure\":\"F1\"})\n",
    "new_frame = pd.concat([d1,d2,d3],axis=1,keys=['Dataset 1',\"Dataset 2\",\"Dataset 3\"]).round(2)\n",
    "print(new_frame.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset 1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset 2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Dataset 3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVCL1</th>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticN</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OurMethod</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BorutaModel</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Dataset 1              Dataset 2              Dataset 3         \\\n",
       "            precision recall    F1 precision recall    F1 precision recall   \n",
       "Ridge            1.00   0.83  0.90      0.98   0.80  0.86      1.00   0.70   \n",
       "SVCL1            0.57   1.00  0.72      0.57   1.00  0.72      0.66   1.00   \n",
       "ElasticN         1.00   0.88  0.93      0.98   0.83  0.89      1.00   0.76   \n",
       "OurMethod        1.00   0.90  0.94      1.00   0.97  0.98      1.00   0.91   \n",
       "BorutaModel      0.99   0.72  0.81      1.00   0.87  0.92      0.98   0.77   \n",
       "\n",
       "                   \n",
       "               F1  \n",
       "Ridge        0.82  \n",
       "SVCL1        0.79  \n",
       "ElasticN     0.85  \n",
       "OurMethod    0.95  \n",
       "BorutaModel  0.86  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
